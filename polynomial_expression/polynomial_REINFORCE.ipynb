{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8cc449a",
   "metadata": {},
   "source": [
    "- state : 하나의 float값 [-3,3]\n",
    "- action : 네트워크가 예측한 y_hat, 실수\n",
    "- reward : `-MSE(y_hat, y_true)` 또는 `-abs(y_hat - y_true)` 형태로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f897f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adffb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다항식 함수 정의 : `y = x^5 + 1.5x^4 - 5x^3 - 7.5x^2 + 4x + 6`\n",
    "\n",
    "coeffs = [1, 1.5, -5.0, -7.5, 4.0, 6.0]\n",
    "\n",
    "def target_function(x):\n",
    "    # x는 torch tensor\n",
    "    powers = torch.stack([x ** i for i in reversed(range(1, 6 + 1))], dim=-1)\n",
    "    coeffs_tensor = torch.tensor(coeffs, dtype=torch.float32).to(x.device)\n",
    "    return (powers * coeffs_tensor).sum(dim=-1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b4d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_head = nn.Linear(64, 1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(1))  # 학습 가능한 표준편차(log)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.net(x)\n",
    "        mu = self.mu_head(features)\n",
    "        std = self.log_std.exp() + 1e-5  # 안정성을 위해 epsilon 추가\n",
    "        return mu, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71d90b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b502634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n",
      "CUDA 사용 가능 여부: True\n",
      "사용 중인 GPU 이름: NVIDIA GeForce RTX 3070\n",
      "Epoch 0, Avg Reward: 0.0000\n",
      "Epoch 100, Avg Reward: -0.0000\n",
      "Epoch 200, Avg Reward: 0.0000\n",
      "Epoch 300, Avg Reward: -0.0000\n",
      "Epoch 400, Avg Reward: 0.0000\n",
      "Epoch 500, Avg Reward: -0.0000\n",
      "Epoch 600, Avg Reward: -0.0000\n",
      "Epoch 700, Avg Reward: -0.0000\n",
      "Epoch 800, Avg Reward: -0.0000\n",
      "Epoch 900, Avg Reward: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def reward_fn(y_pred, y_true):\n",
    "    return -((y_pred - y_true) ** 2)  # MSE 기반 보상\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"사용 중인 디바이스:\", device)\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "if device == 'cuda':\n",
    "    print(\"사용 중인 GPU 이름:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "policy = PolicyNet().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "for epoch in range(1000):  # 에포크 수 조절 가능\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(256):  # 배치 사이즈\n",
    "        x = torch.FloatTensor(1).uniform_(-3, 3).unsqueeze(0).to(device)  # (1, 1)\n",
    "        y_true = target_function(x).to(device)\n",
    "\n",
    "        mu, std = policy(x)\n",
    "        dist = Normal(mu, std)\n",
    "        y_sample = dist.sample()\n",
    "        log_prob = dist.log_prob(y_sample)\n",
    "\n",
    "        reward = reward_fn(y_sample, y_true)\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    rewards = torch.cat(rewards)\n",
    "    \n",
    "    # 보상 정규화\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "\n",
    "    loss = -(log_probs.squeeze() * rewards.squeeze()).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    avg_reward = rewards.mean().item()\n",
    "    reward_history.append(avg_reward)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Avg Reward: {avg_reward:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "\n",
    "x_test = torch.linspace(-3, 3, 1000).unsqueeze(1).to(device)\n",
    "with torch.no_grad():\n",
    "    mu, _ = policy(x_test)\n",
    "    y_pred = mu.cpu().numpy()\n",
    "\n",
    "# 실제 값\n",
    "y_true = np.polyval(coeffs, x_test.cpu().numpy())\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_test.cpu().numpy(), y_true, label=\"Target Function\", color='red')\n",
    "plt.scatter(x_test.cpu().numpy(), y_pred, s=1, label=\"REINFORCE Agent Output\", color='blue')\n",
    "plt.title(\"REINFORCE Agent vs Polynomial Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history)\n",
    "plt.title(\"Reward Progress Over Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_algorithms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
